<!-- The source code is from: https://gkioxari.github.io/ -->


<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>UAV-GESTURE</title>

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
  <link href="../css/style.css" rel="stylesheet" type="text/css" />

<script type="text/javascript" src="../js/hidebib.js"></script>

</head>

<body> 

<div class="container">
  <table border="0" align="center">
    <tr><td width="623" align="center" valign="middle"><h3> UAV-GESTURE: A Dataset for UAV Control and Gesture Recognition</h3></td></tr>
    <tr><td width="300" align="center" valign="middle"> <a href="https://asankagp.github.io/">Asanka G Perera</a>, Yee Wei Law, Javaan Chahl</td></tr>
  </table>
  </br>
  <p><img src="uavgesture.jpg" width="600" align="middle" /></p>
</div>

</br>

<div class="container">
  <h2>Abstract</h2>
    <p>Current UAV-recorded datasets are mostly limited to action recognition and object tracking, whereas the gesture signals datasets were mostly recorded in indoor spaces. Currently, there is no outdoor recorded public video dataset for UAV commanding signals. Gesture signals can be effectively used with UAVs by leveraging the UAVs visual sensors and operational simplicity. To fill this gap and enable research in wider application areas, we present a UAV gesture signals dataset recorded in an outdoor setting. We selected 13 gestures suitable for basic UAV navigation and command from general aircraft handling and helicopter handling signals. We provide 119 high-definition video clips consisting of 37151 frames. The overall baseline gesture recognition performance computed using Pose-based Convolutional Neural Network (P-CNN) is 91.9 %. All the frames are annotated with body joints and gesture classes in order to extend the dataset's applicability to a wider research area including gesture recognition, action recognition, human pose recognition and situation awareness.</p> 
    <tr><a href="https://arxiv.org/pdf/1901.02602.pdf">paper</a></tr>
</div>

</br>

<div class="container" text-align="left">
  <h2>Download</h2>
    <table border="0" align="left">
      <tr><td width="1000" align="left">This dataset is available for academic research only, no commercial use.</br></br> 
       Please send an email to asanka.perera@mymail.unisa.edu.au from your academic email address to get the video dataset download link.</br></br></td></tr>
      <tr><td width="1000" align="left">Download the estimated annotations obtained using OpenPose.</td></tr>
      <tr><td width="1000" align="left"><a href="joint_positions_mat.zip"> Per video Mat files</a></td></tr>
      <tr><td width="1000" align="left"><a href="joint_positions_json.zip"> Per frame JSON files</a></br></br></td></tr>
      <tr><td width="1000" align="left">Download 3 split sets.</td></tr>
      <tr><td width="1000" align="left"><a href="splits.zip"> Splits</a></br></br></td></tr>
      <tr><td width="1000" align="left">If you use this data in your publications please cite our paper "UAV-GESTURE: A Dataset for UAV Control and Gesture Recognition".</td></tr>
    </table> 
</div>

</br>

<div class="containersmall">
  <p>Contact: <a href="mailto:asanka.perera@mymail.unisa.edu.au">Asanka Perera</a></p>
</div>
 


</body>
</html>
