<!-- The source code is from: https://gkioxari.github.io/ -->


<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Aerial Gait Dataset</title>

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
  <link href="../../css/style.css" rel="stylesheet" type="text/css" />

<script type="text/javascript" src="../js/hidebib.js"></script>

</head>

<body> 

<div class="container">
  <table border="0" align="center">
    <tr><td width="700" align="center" valign="middle"><h2>Geladeira</h2></td></tr>
    <!--<tr><td width="300" align="center" valign="middle"> <a href="https://asankagp.github.io/">Asanka G Perera</a>, Yee Wei Law, Javaan Chahl</td></tr>-->
    <tr><td width="300" align="center" valign="middle"> Geladeira Eletrolux Frost Free </tr>
  </table>
  </br>
  <iframe width="560" height="315" src="https://www.youtube.com/embed/EH5yVo2U2nI" frameborder="0" allow="accelerometer; 
  autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  </br></br>

  <p>All videos in the dataset are in HD format (1920x1080). Some videos of this dataset were used for the experiments in our "Dymanic Classifer Selection" paper. 
    We consider the problem of estimating human pose and trajectory by an aerial robot with a monocular camera in near real time. 
    We present a preliminary solution whose distinguishing feature is a dynamic classifier selection architecture. More details can be found in the paper.</br>
    Keywords: Pose estimation, Gait estimation, Trajectory estimation, Human detection, Dynamic classifier selection, UAV, Drone, 
    Perspective distortion</p> 

    <tr>Download paper: 
    <a href="https://arxiv.org/pdf/1812.06408.pdf">arXiv</a> or
    <a href="https://link.springer.com/content/pdf/10.1007%2Fs12559-018-9577-6.pdf">Cogn. Comp. version</a></tr>
</div>
</br>

<div class="container" text-align="left">
  <h2>Download Dataset</h2></br>

    <p><img src="dataset.jpg" width="800" align="middle" /></p></br>
    <table border="0" align="left">
      <tr><td width="1000" align="left">Videos are arranged in three folders.</br></td></tr>

      <tr><td width="1000" align="left"><h3>1. walking on 1 circle</h3>
        Videos were recorded at different heights while the subject was walking on a circle. Videos of two subjects are available.</br>
        S1: (10m, 20m, 30m and 40m) </br>
        S2: (10m, 15m, 20m, 25m, 30m, 35m, 40m and 45m)</td></tr>

      <p><img src="walk_on_circle1.gif" width="400" align="middle" /></p></br>

      <tr><td width="1000" align="left"><h3>2. walking on 2 circles</h3>
        Videos were recorded at 10m height while the subject was walking on two circles (shape of 8).</br></td></tr>

      <tr><td width="1000" align="left"><h3>3. moving camera</h3>
        Videos were recorded while the drone was moving towards and away from the subject. Height was 10m.</br></br></td></tr>

      <tr><td width="1000" align="left">  Download <a href="https://drive.google.com/open?id=1bIbFQON-deoxJodLF3SxX_Xn8acocCBJ">
        all (3.4GB) (17 videos)</a></br>
        OR</br>
        Download individually:</br>
        <a href="https://drive.google.com/open?id=176ULL-NqoRcsj19F1_IEzXihDUrzYq4r">1. walking on 1 circle (1.97GB) (12 videos)</a></br>
        <a href="https://drive.google.com/open?id=11MmlkmmVvnrBjPf-5I6mC9MI1bQ4YLTm">2. walking on 2 circles (0.67GB) (2 videos)</a></br> 
        <a href="https://drive.google.com/open?id=1Ah77qFaJIvOwuTLrh7-pSeVzcLmVH9tB">3. moving camera (0.78GB) (3 videos)</a></td></tr>
    </table> 
</div>

</br>

<div class="containersmall">
  <h4>Reference:</h4>
  <p>A. G. Perera, Y. W. Law, and J. Chahl, “Human pose and path estimation from aerial video using dynamic classifier selection,”
  Cognitive Computation, Jun 2018. [Online]. Available: https://doi.org/ 10.1007/s12559-018-9577-6</p>
  <p>Contact: <a href="mailto:asanka.perera@mymail.unisa.edu.au">Asanka Perera</a></p>
</div>
 


</body>
</html>
